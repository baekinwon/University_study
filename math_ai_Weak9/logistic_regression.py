# -*- coding: utf-8 -*-
"""Logistic regression.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1dxgSZEjdx5UKY1oNGleKjo8w3aQNAFOf
"""

import numpy as np

# 학습데이터 준비
x_data = np.array([2,4,6,8,10,12,14,16,18,20]).reshape(10,1)
t_data = np.array([0,0,0,0,0,0,1,1,1,1]).reshape(10,1)

# 임의의 직선 z= Wx+b 정의
W = np.random.rand(1,1)
b = np.random.rand(1)

print(x_data.shape,t_data.shape)
print(W,W.shape,b,b.shape)

# 크로스 엔트로피 정의
def sigmoid(z):
    return 1/(1+np.exp(-z))

def loss_func(x,t):
    delta = 1e-7 # log 무한대 발산 방지 log0이 되면 안되기 때문임
    z = np.dot(x,W)+b
    y = sigmoid(z)

    return -np.sum(t*np.log(y+delta)+(1-t)*np.log((1-y)+delta))

# 수치미분 numerical_derivative 및 utility 함수 정의 
def numerical_derivative(f,x): # f는 미분하고자 하는 다변수 함수, x는 모든 변수를 포함하고 있는 numpy 객체(배열, 행렬)등
    delta_x = 1e-5 #lim에 해당되는 작은 값
    grad = np.zeros_like(x) # 계산된 수치미분 값 저장 변수

    it = np.nditer(x, flags =['multi_index'],op_flags=['readwrite'])# 모든 입력변수에 대해 편미분하기 위해 사용
    while not it.finished:
        idx = it.multi_index  # x에대한 편미분 후 y에 대한 편미분 실행  [1.0,2.0]이라면 1.0 편미분 후 2.0 편미분
        
        tmp_val = x[idx] # numpy 타입은 mutable이므로 원래 값 보관
        x[idx] = float(tmp_val)+delta_x #하나의 변수에 대해 수치미분 계산
        fx1 = f(x) # f(x+delta_x) 전체에 대해 계산해야 하기 때문에 x[idx]가 아닌 x를 넣어줌

        x[idx] = tmp_val - delta_x
        fx2=f(x) # f(x-delta_x)
        grad[idx] = (fx1-fx2)/(2*delta_x)

        x[idx] = tmp_val
        it.iternext()

    return grad

def loss_val(x,t):
    delta = 1e-7 # log 무한대 발산 방지 log0이 되면 안되기 때문임
    z = np.dot(x,W)+b
    y = sigmoid(z)

    return -np.sum(t*np.log(y+delta)+(1-t)*np.log((1-y)+delta))

def predict(test_data):
    z = np.dot(test_data,W)+b
    y = sigmoid(z)

    if y>=0.5:
        result = 1
    else:
        result = 0
        
    return y,result

# 학습률 초기화 및 손실함수가 최소가 될 때까지 W,b 업데이트
learning_rate = 1e-2 

f = lambda x: loss_func(x_data,t_data) # loss_func에 x_data,t_data가 들어감
print("Initial loss value = ",loss_val(x_data,t_data))
for step in range(50001):
    W-= learning_rate * numerical_derivative(f,W) # w에 대한 수치미분
    b-= learning_rate * numerical_derivative(f,b) # b에 대한 수치미분

    if(step % 5000==0):
        print("step = ",step,"loss value = ",loss_val(x_data,t_data))

# 3과 17에 대한 미래 값 예측
test_data = np.array([3.0])
(real_val_1,logical_val_1) = predict(test_data)
print(real_val_1,logical_val_1)

test_data = np.array([17.0])
(real_val_1,logical_val_1) = predict(test_data)
print(real_val_1,logical_val_1)